{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGOKPSv0esi7mnXbrlUwLQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sofa3xpert/CW2_ML/blob/main/CW2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rJo06K3tsCWl"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# Module: env_setup.py\n",
        "########################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Utility classes for logging\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.sum = 0.0\n",
        "        self.count = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.sum / self.count if self.count != 0 else 0.0\n",
        "\n",
        "class Metric:\n",
        "    def __init__(self):\n",
        "        self.correct = 0\n",
        "        self.total = 0\n",
        "    def update_prediction(self, preds, targets):\n",
        "        _, predicted = torch.max(preds, 1)\n",
        "        self.correct += (predicted == targets).sum().item()\n",
        "        self.total += targets.size(0)\n",
        "    def calc_accuracy(self):\n",
        "        return self.correct / self.total if self.total != 0 else 0.0\n",
        "\n",
        "def mapping_func(name):\n",
        "    # Example convex mapping: M(x) = x/(2-x) for x in [0,1]\n",
        "    if name == \"convex\":\n",
        "        return lambda beta: beta/(2-beta) if beta < 2 else 1.0\n",
        "    elif name == \"linear\":\n",
        "        return lambda beta: beta\n",
        "    else:\n",
        "        return lambda beta: beta\n",
        "\n",
        "def evaluate_step(model, dataloader, device):\n",
        "    model.eval()\n",
        "    metric = Metric()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            metric.update_prediction(outputs, labels.to(device))\n",
        "    return metric.calc_accuracy()\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay, device):\n",
        "        self.shadow = {}\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "    def update(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n",
        "    def apply_shadow(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = self.shadow[name].clone()\n",
        "\n",
        "# Experiment arguments\n",
        "class ExperimentArgs:\n",
        "    def __init__(self):\n",
        "        # Network and dataset parameters:\n",
        "        self.network = \"resnet18\"           # or \"resnet50\" (change for experimentation purposes only)\n",
        "        self.num_classes = 10               # CIFAR-10\n",
        "        self.data = \"CIFAR10\"\n",
        "        self.num_X = 0                      # Start with an empty labeled set (L₀ = ∅)\n",
        "        self.include_x_in_u = True\n",
        "        self.augs = None                    # Augmentations are defined in the data module\n",
        "        self.batch_size = 64\n",
        "        self.mu = 7\n",
        "\n",
        "        # Semi-supervised training parameters:\n",
        "        self.lr = 0.03                    # Learning rate for SSL training\n",
        "        self.momentum = 0.9\n",
        "        self.nesterov = True\n",
        "        self.weight_decay = 5e-4\n",
        "        self.iterations = 1048576         # Total training iterations (~2^20)\n",
        "\n",
        "        ### WARNING: num of epochs reduced for limited session time in Colab ####\n",
        "\n",
        "        self.epochs_semi = 10             # Number of epochs per active learning round\n",
        "\n",
        "        # Supervised training parameters (for linear evaluation):\n",
        "\n",
        "\n",
        "            ### WARNING: num of epochs reduced for limited session time in Colab ####\n",
        "        self.epochs_supervised = 10\n",
        "\n",
        "        # Other training fields:\n",
        "        self.wandb = False\n",
        "        self.mode = \"train\"\n",
        "        self.load_path = \"ckpt.pth\"       # For SSL training checkpoint saving/loading\n",
        "        self.ema_decay = 0.999\n",
        "        self.amp = False                  # Set True if using Automatic Mixed Precision\n",
        "\n",
        "        # Thresholding parameters:\n",
        "        self.mapping = \"convex\"\n",
        "        self.threshold = 0.95\n",
        "        self.lu_weight = 1.0\n",
        "        self.save_path = \"./checkpoints\"\n",
        "\n",
        "        # Pre-trained representation checkpoint:\n",
        "        # For ResNet18, checkpoint should be for SimCLR with ResNet18.\n",
        "        # For ResNet50, update accordingly.\n",
        "        self.simclr_checkpoint = \"/content/simclr_cifar-10.pth.tar\"\n",
        "        self.use_typiclust_initial = True\n",
        "\n",
        "        # Active Learning parameters:\n",
        "        self.initial_size = 0             # L₀ is empty.\n",
        "        self.budget = 10                  # Query 10 samples per round.\n",
        "        self.active_rounds = 10           # Total active learning rounds.\n",
        "\n",
        "        # For PPL or learning_status storage:\n",
        "        self.learning_status = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.__dict__)\n",
        "\n",
        "# Create instance of ExperimentArgs\n",
        "exp_args = ExperimentArgs()\n",
        "\n",
        "# Create checkpoint directory if it doesn't exist.\n",
        "os.makedirs(exp_args.save_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Module: representation.py\n",
        "########################################\n",
        "# This module implements SimCLR-based representation learning functions\n",
        "# for both ResNet18 and ResNet50.\n",
        "\n",
        "# MLP projection head: we reuse the one from env_setup\n",
        "class MLPProjection(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super(MLPProjection, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# SimCLR model wrapper\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_model, projector):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder = base_model      # Produces features (dim depends on model)\n",
        "        self.projector = projector     # Maps features to 128-dim projection\n",
        "    def forward(self, x, return_features=False):\n",
        "        features = self.encoder(x)\n",
        "        if return_features:\n",
        "            return F.normalize(features, dim=1)\n",
        "        projection = self.projector(features)\n",
        "        return F.normalize(projection, dim=1)\n",
        "\n",
        "def get_simclr_model(network_type, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Returns a pre-trained SimCLR model.\n",
        "    For 'resnet18': use a modified ResNet18 with 512-dim output.\n",
        "    For 'resnet50': use a modified ResNet50 with 2048-dim output.\n",
        "    \"\"\"\n",
        "    if network_type.lower() == \"resnet18\":\n",
        "        base_model = models.resnet18(weights=None)\n",
        "        base_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        base_model.maxpool = nn.Identity()\n",
        "        base_model.fc = nn.Identity()  # Output: 512-dim features.\n",
        "        in_dim = 512\n",
        "    elif network_type.lower() == \"resnet50\":\n",
        "        base_model = models.resnet50(weights=None)\n",
        "        base_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        base_model.maxpool = nn.Identity()\n",
        "        base_model.fc = nn.Identity()  # Output: 2048-dim features.\n",
        "        in_dim = 2048\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported network type\")\n",
        "\n",
        "    projector = MLPProjection(in_dim=in_dim, hidden_dim=in_dim, out_dim=128)\n",
        "    model = SimCLR(base_model, projector)\n",
        "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_embeddings(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Extracts L2-normalized embeddings from the encoder (penultimate layer).\n",
        "    \"\"\"\n",
        "    embeddings, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for sample in dataloader:\n",
        "            # Check if the sample tuple has 3 elements.\n",
        "            if len(sample) == 3:\n",
        "                imgs, lbls, _ = sample\n",
        "            else:\n",
        "                imgs, lbls = sample\n",
        "            imgs = imgs.to(device)\n",
        "            feats = model(imgs, return_features=True)\n",
        "            embeddings.append(feats.cpu().numpy())\n",
        "            labels.extend(lbls.numpy())\n",
        "    embeddings = np.concatenate(embeddings, axis=0)\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    embeddings = embeddings / (norms + 1e-10)\n",
        "    return embeddings, np.array(labels)"
      ],
      "metadata": {
        "id": "asDP7adgsHNi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% #### Data Preparation and Indexed Dataset for CIFAR-10\n",
        "from torchvision import datasets\n",
        "\n",
        "# Define transforms.\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# Standard CIFAR-10 for labeled set.\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "# Custom dataset for unlabeled pool that returns (image, label, index).\n",
        "class IndexedCIFAR10(datasets.CIFAR10):\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super(IndexedCIFAR10, self).__getitem__(index)\n",
        "        return img, target, index\n",
        "\n",
        "unlabeled_dataset = IndexedCIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "def get_dataloaders(data, num_X, include_x_in_u, batch_size, mu):\n",
        "    U_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    T_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return None, U_loader, T_loader"
      ],
      "metadata": {
        "id": "wOyOtuqPvuyc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Module: active_selection.py\n",
        "########################################\n",
        "def original_typiclust_selection(embeddings, unlabeled_indices, budget):\n",
        "    \"\"\"\n",
        "    Original TypiClust selection:\n",
        "    - Uses KMeans with n_clusters = budget,\n",
        "    - Selects the sample closest to each cluster centroid.\n",
        "    \"\"\"\n",
        "    # Cluster the embeddings into 'budget' clusters\n",
        "    kmeans = KMeans(n_clusters=budget, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    selected_local = []  # To store indices (relative to embeddings) selected from each cluster\n",
        "\n",
        "    # Define a default number of neighbors for typicality calculation.\n",
        "    # This can be tuned based on the dataset.\n",
        "    default_k = 5\n",
        "\n",
        "    # Process each cluster\n",
        "    for cluster in range(budget):\n",
        "        # Get indices of points belonging to the current cluster\n",
        "        indices = np.where(cluster_labels == cluster)[0]\n",
        "        cluster_points = embeddings[indices]\n",
        "\n",
        "        # Edge case: If a cluster has no points (should not happen in KMeans)\n",
        "        if len(indices) == 0:\n",
        "            continue\n",
        "\n",
        "        # If there is only one point in the cluster, select it automatically.\n",
        "        if len(indices) == 1:\n",
        "            selected_local.append(indices[0])\n",
        "        else:\n",
        "            # Determine the number of neighbors: we use (default_k + 1) because the point itself\n",
        "            # is always the nearest neighbor (with distance 0). If the cluster is smaller than\n",
        "            # (default_k + 1), use the available number.\n",
        "            n_neighbors = min(default_k + 1, len(indices))\n",
        "\n",
        "            # Compute nearest neighbors within the cluster.\n",
        "            # Note: metric 'euclidean' is used to compute the Euclidean distances.\n",
        "            nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean').fit(cluster_points)\n",
        "            distances, neighbors = nbrs.kneighbors(cluster_points)\n",
        "            # distances is a 2D array of shape (n_points_in_cluster, n_neighbors)\n",
        "\n",
        "            # Calculate typicality for each point.\n",
        "            # Exclude the first neighbor (the point itself, with distance 0) if there is more than one neighbor.\n",
        "            if n_neighbors > 1:\n",
        "                # Compute the average of the inverse distances for the K nearest neighbors (excluding itself)\n",
        "                typicality_scores = np.mean(1.0 / distances[:, 1:], axis=1)\n",
        "            else:\n",
        "                typicality_scores = np.zeros(len(indices))\n",
        "\n",
        "            # Choose the point with the highest typicality score.\n",
        "            best_idx_local = np.argmax(typicality_scores)\n",
        "            selected_local.append(indices[best_idx_local])\n",
        "\n",
        "    # Map the selected local indices back to the global indices from the original unlabeled pool.\n",
        "    selected_local = np.array(selected_local)\n",
        "    selected_global = np.array(unlabeled_indices)[selected_local]\n",
        "\n",
        "    return selected_global\n",
        "\n",
        "def modified_active_selection(embeddings, unlabeled_indices, labeled_count, query_budget, max_clusters=500, neighbor_k=20, min_cluster_size=5):\n",
        "    \"\"\"\n",
        "    Modified active selection:\n",
        "    - Set K = min(|L_{i-1}| + B, max_clusters)\n",
        "    - For each cluster with at least min_cluster_size samples,\n",
        "      compute typicality as the inverse of the average Euclidean distance to\n",
        "      its min{neighbor_k, cluster_size} nearest neighbors.\n",
        "    - Select the top query_budget samples across all clusters based on typicality.\n",
        "    \"\"\"\n",
        "    K = min(labeled_count + query_budget, max_clusters)\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=K, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    typicality_scores = []\n",
        "    corresponding_indices = []\n",
        "    for cluster in range(K):\n",
        "        cluster_idxs = np.where(cluster_labels == cluster)[0]\n",
        "        if len(cluster_idxs) < min_cluster_size:\n",
        "            continue  # Skip small clusters.\n",
        "        cluster_points = embeddings[cluster_idxs]\n",
        "        k_neighbors = min(neighbor_k, len(cluster_points))\n",
        "        for i, idx in enumerate(cluster_idxs):\n",
        "            point = cluster_points[i]\n",
        "            distances = np.linalg.norm(cluster_points - point, axis=1)\n",
        "            distances = np.delete(distances, i)\n",
        "            avg_dist = np.mean(np.sort(distances)[:k_neighbors]) if len(distances) > 0 else np.inf\n",
        "            typicality = 1.0 / (avg_dist + 1e-8)\n",
        "            typicality_scores.append(typicality)\n",
        "            corresponding_indices.append(unlabeled_indices[idx])\n",
        "\n",
        "    if len(typicality_scores) < query_budget:\n",
        "        selected_global = np.array(corresponding_indices)\n",
        "    else:\n",
        "        typicality_scores = np.array(typicality_scores)\n",
        "        selected_idx_local = np.argsort(-typicality_scores)[:query_budget]\n",
        "        selected_global = np.array(corresponding_indices)[selected_idx_local]\n",
        "    return selected_global"
      ],
      "metadata": {
        "id": "vfT0monQsML6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def active_learning_experiment(args, selection_method=\"original_typiclust\"):\n",
        "    \"\"\"\n",
        "    Active Learning experiment that integrates the selection method.\n",
        "\n",
        "    Parameters:\n",
        "      args: ExperimentArgs instance.\n",
        "      selection_method: String; one of \"modified_typiclust\", \"original_typiclust\", or \"random\".\n",
        "\n",
        "    Returns:\n",
        "      test_accuracies: List of test accuracies for each active learning round.\n",
        "      cumulative_budgets: List of cumulative labeled sample counts after each round.\n",
        "      labeled_indices: List of global indices selected for labeling.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Get dataloaders for unlabeled pool and test set.\n",
        "    _, U_loader, T_loader = get_dataloaders(args.data, args.num_X, args.include_x_in_u, args.batch_size, args.mu)\n",
        "    total_unlabeled = len(train_dataset)\n",
        "    labeled_mask = np.zeros(total_unlabeled, dtype=bool)\n",
        "    labeled_indices = []\n",
        "    cumulative_budgets = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # Load pre-trained SimCLR model for feature extraction.\n",
        "    simclr_model = get_simclr_model(args.network, args.simclr_checkpoint)\n",
        "    simclr_model.to(device)\n",
        "\n",
        "    for round in range(args.active_rounds):\n",
        "        print(f\"\\nActive Learning Round {round+1}/{args.active_rounds}\")\n",
        "        unlabeled_indices = np.where(~labeled_mask)[0]\n",
        "        U_subset = Subset(unlabeled_dataset, unlabeled_indices)\n",
        "        U_subset_loader = DataLoader(U_subset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        # Extract embeddings from the current unlabeled pool.\n",
        "        embeddings, _ = extract_embeddings(simclr_model, U_subset_loader, device)\n",
        "\n",
        "        if selection_method == \"modified_typiclust\":\n",
        "            current_label_count = len(labeled_indices)\n",
        "            selected_global = modified_active_selection(embeddings, unlabeled_indices, current_label_count, args.budget)\n",
        "            print(\"Modified TypiClust selected indices:\", selected_global)\n",
        "        elif selection_method == \"original_typiclust\":\n",
        "            selected_global = original_typiclust_selection(embeddings, unlabeled_indices, args.budget)\n",
        "            print(\"Original TypiClust selected indices:\", selected_global)\n",
        "        elif selection_method == \"random\":\n",
        "            selected_global = np.random.choice(unlabeled_indices, size=args.budget, replace=False)\n",
        "            print(\"Random selected indices:\", selected_global)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown selection method\")\n",
        "\n",
        "        # Update labeled set.\n",
        "        labeled_mask[selected_global] = True\n",
        "        labeled_indices.extend(selected_global.tolist())\n",
        "        print(\"Total labeled samples so far:\", len(labeled_indices))\n",
        "        cumulative_budgets.append(len(labeled_indices))\n",
        "\n",
        "        # Build labeled dataloader from train_dataset.\n",
        "        X_subset = Subset(train_dataset, labeled_indices)\n",
        "        X_loader_current = DataLoader(X_subset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        # Run full semi-supervised training for this round.\n",
        "        print(\"Starting full semi-supervised training for this round...\")\n",
        "        test_acc = train_semi_supervised_full(args, X_loader_current, U_loader, T_loader, device, simclr_model, network_type=args.network)\n",
        "        test_accuracies.append(test_acc)\n",
        "        print(f\"After round {round+1}: Test Accuracy = {test_acc*100:.2f}%\")\n",
        "\n",
        "    return test_accuracies, cumulative_budgets, labeled_indices"
      ],
      "metadata": {
        "id": "mZaQYykH8PdQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# Module: supervised_training.py\n",
        "########################################\n",
        "def get_network(network_type, num_classes):\n",
        "    # Builds a modified ResNet for CIFAR-10.\n",
        "    if network_type.lower() == \"resnet18\":\n",
        "        net = models.resnet18(weights=None, num_classes=num_classes)\n",
        "        net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        net.maxpool = nn.Identity()\n",
        "        return net\n",
        "    elif network_type.lower() == \"resnet50\":\n",
        "        net = models.resnet50(weights=None, num_classes=num_classes)\n",
        "        net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        net.maxpool = nn.Identity()\n",
        "        return net\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported network type\")\n",
        "\n",
        "def train_supervised(args, device, network_type):\n",
        "    model = get_network(network_type, args.num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.025, momentum=0.9,\n",
        "                                nesterov=True, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.iterations)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_acc = []\n",
        "    for epoch in range(args.epochs_supervised):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "        scheduler.step()\n",
        "        avg_loss = epoch_loss / len(train_dataset)\n",
        "        test_acc = evaluate_step(model, test_loader, device)\n",
        "        epoch_losses.append(avg_loss)\n",
        "        epoch_acc.append(test_acc)\n",
        "        print(f\"[Supervised {network_type}] Epoch {epoch+1}/{args.epochs_supervised}: Loss = {avg_loss:.4f}, Test Acc = {test_acc*100:.2f}%\")\n",
        "\n",
        "    # Save checkpoint.\n",
        "    torch.save(model.state_dict(), os.path.join(args.save_path, f\"supervised_{network_type}.pth\"))\n",
        "    return model, epoch_losses, epoch_acc\n",
        "\n",
        "def supervised_with_ss_evaluation(args, device, network_type):\n",
        "    # Load pre-trained SimCLR model for given network type.\n",
        "    simclr_model = get_simclr_model(network_type, args.simclr_checkpoint)\n",
        "    simclr_model.to(device)\n",
        "\n",
        "    # Build dataloaders.\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Extract embeddings.\n",
        "    train_emb, train_labels = extract_embeddings(simclr_model, train_loader, device)\n",
        "    test_emb, test_labels = extract_embeddings(simclr_model, test_loader, device)\n",
        "\n",
        "    # For ResNet18, embedding dim=512; for ResNet50, dim=2048.\n",
        "    d = 512 if network_type.lower() == \"resnet18\" else 2048\n",
        "    import torch.optim as optim\n",
        "    emb_tensor = torch.tensor(train_emb, dtype=torch.float32)\n",
        "    lbl_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "    dataset = torch.utils.data.TensorDataset(emb_tensor, lbl_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    classifier = nn.Linear(d, args.num_classes).to(device)\n",
        "    optimizer = optim.SGD(classifier.parameters(), lr=2.5, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_epochs = args.epochs_supervised * 2\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        epoch_loss = 0.0\n",
        "        for feats, lbls in loader:\n",
        "            feats = feats.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = classifier(feats)\n",
        "            loss = criterion(outputs, lbls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * feats.size(0)\n",
        "        scheduler.step()\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        test_tensor = torch.tensor(test_emb, dtype=torch.float32).to(device)\n",
        "        outputs = classifier(test_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_acc = (predicted.cpu() == torch.tensor(test_labels)).float().mean().item()\n",
        "    print(f\"[Linear Evaluation {network_type}] Test Accuracy: {test_acc*100:.2f}%\")\n",
        "    # Save checkpoint\n",
        "    torch.save(classifier.state_dict(), os.path.join(args.save_path, f\"linear_{network_type}.pth\"))\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "QLpb9uHUsO5K"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Module: semi_supervised.py\n",
        "########################################\n",
        "def train_semi_supervised_full(args, X_loader, U_loader, T_loader, device, simclr_model, network_type=\"resnet18\"):\n",
        "    \"\"\"\n",
        "    Full semi-supervised training loop (FlexMatch-style) without the Prior Pseudo-Label (PPL)\n",
        "    mechanism. Instead, it uses the network's own pseudo-labels from weak augmented unlabeled data.\n",
        "\n",
        "    Parameters:\n",
        "      args: ExperimentArgs instance.\n",
        "      X_loader: Labeled dataloader.\n",
        "      U_loader: Unlabeled dataloader (should yield (img, label, index)).\n",
        "      T_loader: Test dataloader.\n",
        "      device: torch.device.\n",
        "      simclr_model: Pre-trained SimCLR model (not used here but kept for interface consistency).\n",
        "      network_type: \"resnet18\" or \"resnet50\".\n",
        "\n",
        "    Returns:\n",
        "      test_acc: Final test accuracy after training.\n",
        "    \"\"\"\n",
        "    model = get_network(network_type, args.num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum,\n",
        "                                nesterov=args.nesterov, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.iterations)\n",
        "    scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
        "    ema = EMA(model, args.ema_decay, device)\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # No PPL, so we don't compute a PPL dictionary.\n",
        "    global_step = 0\n",
        "    for epoch in range(args.epochs_semi):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for sample_x, sample_u in zip(X_loader, U_loader):\n",
        "            # Labeled batch: (img, label)\n",
        "            x, y = sample_x\n",
        "            # Unlabeled batch: (img, label, index); label is dummy.\n",
        "            uw, _, u_indices = sample_u\n",
        "            # Simulate strong augmentation; in practice, use a different strong augmentation.\n",
        "            us = uw.clone()\n",
        "\n",
        "            # Forward pass for all concatenated inputs.\n",
        "            inputs = torch.cat([x, uw, us], dim=0)\n",
        "            outputs = model(inputs.to(device))\n",
        "            bs = x.size(0)\n",
        "            bs_u = uw.size(0)\n",
        "            xw_pred = outputs[:bs]\n",
        "            uw_pred = outputs[bs:bs+bs_u]\n",
        "            us_pred = outputs[bs+bs_u:]\n",
        "\n",
        "            # Supervised loss.\n",
        "            ls = criterion(xw_pred, y.to(device)).mean()\n",
        "            total_loss = ls\n",
        "\n",
        "            # Conventional pseudo-labeling: use network's own predictions on weak augmentation.\n",
        "            with torch.no_grad():\n",
        "                uw_prob = F.softmax(uw_pred, dim=1)\n",
        "                max_prob, hard_label = torch.max(uw_prob, dim=1)\n",
        "                indicator = max_prob > args.threshold  # Binary mask of high confidence predictions.\n",
        "            # Compute unsupervised loss on strong augmented images.\n",
        "            lu = (criterion(us_pred, hard_label) * indicator.float()).mean()\n",
        "            total_loss += args.lu_weight * lu\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            if args.amp:\n",
        "                scaler.scale(total_loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "            scheduler.step()\n",
        "            ema.update(model)\n",
        "            global_step += 1\n",
        "            epoch_loss += total_loss.item()\n",
        "        test_acc = evaluate_step(model, T_loader, device)\n",
        "        print(f\"[Semi-supervised No PPL {network_type}] Epoch {epoch+1}/{args.epochs_semi}: Loss = {epoch_loss:.4f}, Test Acc = {test_acc*100:.2f}%\")\n",
        "    # Save the checkpoint.\n",
        "    torch.save(model.state_dict(), os.path.join(args.save_path, f\"semi_supervised_no_PPL_{network_type}.pth\"))\n",
        "    return test_acc"
      ],
      "metadata": {
        "id": "itN0gl_vsUNz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Module: plotting.py\n",
        "########################################\n",
        "def plot_accuracy_vs_budget(cum_budgets, accuracies, title=\"Test Accuracy vs. Cumulative Labeled Samples\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(cum_budgets, np.array(accuracies)*100, marker='o', label='Test Accuracy')\n",
        "    plt.xlabel(\"Cumulative Labeled Samples\")\n",
        "    plt.ylabel(\"Test Accuracy (%)\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_epoch_time(method_names, epoch_times):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.bar(method_names, epoch_times, color=['blue', 'red', 'green'])\n",
        "    plt.xlabel(\"Method\")\n",
        "    plt.ylabel(\"Average Epoch Time (s)\")\n",
        "    plt.title(\"Average Epoch Time Comparison\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-1NTn77qsXfk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Module: checkpoint.py\n",
        "########################################\n",
        "def save_checkpoint(model, optimizer, epoch, filepath):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "\n",
        "def load_checkpoint(model, optimizer, filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    return model, optimizer, start_epoch"
      ],
      "metadata": {
        "id": "E5sRooNisbI7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Final Experiment Pipeline for Supervised Embeddings and Semi-Supervised Training\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Fully Supervised (Linear Evaluation)\n",
        "# -------------------------------\n",
        "print(\"=== Running Fully Supervised (Linear Evaluation) for ResNet18 ===\")\n",
        "# This function loads a pre-trained SimCLR-ResNet18 model, extracts L2-normalized embeddings from the penultimate layer,\n",
        "# trains a linear classifier on top, evaluates it, and saves the classifier checkpoint.\n",
        "model_fs, losses, accuracy = train_supervised(exp_args, device, \"resnet18\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Fully Supervised with Self-Supervised Embeddings (Linear Evaluation)\n",
        "# -------------------------------\n",
        "print(\"=== Running Fully Supervised with Self-Supervised Embeddings (Linear Evaluation) for ResNet18 ===\")\n",
        "# This function loads a pre-trained SimCLR-ResNet18 model, extracts L2-normalized embeddings from the penultimate layer,\n",
        "# trains a linear classifier on top, evaluates it, and saves the classifier checkpoint.\n",
        "classifier_ss = supervised_with_ss_evaluation(exp_args, device, network_type=\"resnet18\")\n",
        "\n",
        "test_acc_rand, cum_budget_rand, labeled_rand = active_learning_experiment(exp_args, selection_method=\"random\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Semi-Supervised Training (FlexMatch-style with PPL) within Active Learning\n",
        "# -------------------------------\n",
        "print(\"=== Running Active Learning Experiment: original_typiclust (Semi-Supervised Training) for ResNet18 ===\")\n",
        "# This active learning experiment integrates our modified active selection (dynamic clustering and typicality computation)\n",
        "# with our full semi-supervised training loop (FlexMatch).\n",
        "test_acc_mod, cum_budget_mod, labeled_mod = active_learning_experiment(exp_args, selection_method=\"original_typiclust\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Plotting Results\n",
        "# -------------------------------\n",
        "# Use our plotting functions to visualize the performance as a function of cumulative budget.\n",
        "# Plotting the test accuracy progression vs. cumulative labeled samples.\n",
        "plot_accuracy_vs_budget(cum_budget_mod, test_acc_mod, title=\"Test Accuracy vs. Cumulative Labeled Samples (Modified TypiClust)\")\n",
        "plot_accuracy_vs_budget(cum_budget_rand, test_acc_rand, title=\"Test Accuracy vs. Cumulative Labeled Samples (Random Selection)\")\n",
        "\n",
        "final_acc_mod = test_acc_mod[-1]\n",
        "final_acc_rand = test_acc_rand[-1]\n",
        "methods = [\"Modified TypiClust\", \"Random Selection\"]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(methods, [final_acc_mod*100, final_acc_rand*100], color=['blue','red'])\n",
        "plt.xlabel(\"Active Learning Selection Method\")\n",
        "plt.ylabel(\"Final Test Accuracy (%)\")\n",
        "plt.title(\"Final Test Accuracy Comparison\")\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(True, axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w2tF6G78zqE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}